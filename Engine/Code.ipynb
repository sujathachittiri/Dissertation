{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0TquGrb9J1uYMXijOGdyE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujathachittiri/Dissertation/blob/main/Engine/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a realistic merged regulatory-style dataset (~7000 rows) inspired by open-source financial datasets (BankSim, UCI Credit, Kaggle loans)\n",
        "# and structured to resemble regulatory AnaCredit-like fields. This file is saved to /mnt/data/realistic_regulatory_data.csv\n",
        "# The dataset is synthetic but contains realistic distributions, categorical diversity, and injected anomalies.\n",
        "# This code is Colab/local-friendly and doesn't use ace_tools.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random, os\n",
        "\n",
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "\n",
        "N = 7000  # medium size dataset\n",
        "\n",
        "# Account IDs\n",
        "account_ids = [f\"CUST-{500000 + i}\" for i in range(N)]\n",
        "\n",
        "# Report dates spanning 2023-2025 quarterly periods\n",
        "start = datetime(2023,1,1)\n",
        "report_dates = [start + timedelta(days=int(x)) for x in np.random.randint(0, 1000, size=N)]\n",
        "report_dates = pd.to_datetime(report_dates).date\n",
        "\n",
        "# Exposure amounts: mix of lognormal and a portion from loan-like distributions\n",
        "exposure_amount = np.round(np.where(\n",
        "    np.random.rand(N) < 0.7,\n",
        "    np.random.lognormal(mean=12, sigma=1.0, size=N),          # typical exposures\n",
        "    np.random.normal(loc=2000000, scale=800000, size=N)      # larger corporate exposures\n",
        ")).astype(int)\n",
        "exposure_amount = np.where(exposure_amount < 100, np.abs(exposure_amount) + 100, exposure_amount)\n",
        "\n",
        "# Counterparty types and derived risk weights\n",
        "counterparty_types = np.random.choice(['Bank','Corporate','Retail','Sovereign','SME'], size=N, p=[0.15,0.35,0.35,0.03,0.12])\n",
        "\n",
        "# Country codes and currency mapping (adds realism: multiple currencies)\n",
        "country_codes = np.random.choice(['GB','DE','HK','US','FR','IN','SG','ES','IT','NL'], size=N,\n",
        "                                 p=[0.15,0.12,0.10,0.15,0.10,0.12,0.06,0.05,0.08,0.07])\n",
        "currency_map = {'IN':'INR','GB':'GBP','DE':'EUR','HK':'HKD','US':'USD','FR':'EUR','SG':'SGD','ES':'EUR','IT':'EUR','NL':'EUR'}\n",
        "currency = [currency_map[c] for c in country_codes]\n",
        "\n",
        "# Credit score proxies (0-850) for retail/SME; NaN for corporates/sov/banks\n",
        "credit_score = np.where(np.isin(counterparty_types, ['Retail','SME']),\n",
        "                        np.random.normal(loc=650, scale=80, size=N).astype(int), np.nan)\n",
        "credit_score = np.clip(credit_score, 300, 850)\n",
        "credit_score = np.where(np.isnan(credit_score), None, credit_score)\n",
        "\n",
        "# Tenor (months) and transaction count\n",
        "tenor_months = np.random.choice([6,12,24,36,48,60,120], size=N, p=[0.05,0.1,0.2,0.25,0.2,0.15,0.05])\n",
        "transaction_count = np.random.poisson(lam=4, size=N)\n",
        "\n",
        "# Risk weight baseline by counterparty type (simplified realistic mapping)\n",
        "rw_map = {'Retail': 0.35, 'Corporate': 0.5, 'Bank': 0.2, 'Sovereign': 0.0, 'SME': 0.75}\n",
        "risk_weight = np.array([rw_map[ct] for ct in counterparty_types])\n",
        "\n",
        "# Introduce some country-specific risk upweighting (e.g., EM countries higher RW)\n",
        "country_up = {'IN': 1.2, 'HK': 1.0, 'US': 1.0, 'GB': 1.0, 'DE': 1.0, 'FR':1.0, 'SG':1.0, 'ES':1.0, 'IT':1.1, 'NL':1.0}\n",
        "risk_weight = risk_weight * np.array([country_up[c] for c in country_codes])\n",
        "# Clip risk weight to max 1.0\n",
        "risk_weight = np.clip(risk_weight, 0.0, 1.0).round(2)\n",
        "\n",
        "# Capital requirement: simple regulatory-like formula (Exposure * RW * 8%)\n",
        "capital_requirement = (exposure_amount * risk_weight * 0.08).round(0).astype(int)\n",
        "\n",
        "# Source system / origin (to mimic integration scenarios)\n",
        "source_system = np.random.choice(['CoreBank','LoanServicing','Treasury','TradeRepo','ExternalFeed'], size=N, p=[0.4,0.25,0.15,0.1,0.1])\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Account_ID': account_ids,\n",
        "    'Report_Date': report_dates,\n",
        "    'Exposure_Amount': exposure_amount,\n",
        "    'Risk_Weight': risk_weight,\n",
        "    'Capital_Requirement': capital_requirement,\n",
        "    'Country_Code': country_codes,\n",
        "    'Currency': currency,\n",
        "    'Counterparty_Type': counterparty_types,\n",
        "    'Tenor_Months': tenor_months,\n",
        "    'Transaction_Count': transaction_count,\n",
        "    'Source_System': source_system,\n",
        "    'Credit_Score': credit_score\n",
        "})\n",
        "\n",
        "# Add derived/regulatory-like fields (e.g., Product Type, Facility ID)\n",
        "df['Product_Type'] = np.random.choice(['TermLoan','Overdraft','CashCredit','TradeFinance','Repo'], size=N, p=[0.3,0.25,0.2,0.15,0.1])\n",
        "df['Facility_ID'] = ['FAC-' + str(700000 + i) for i in range(len(df))]\n",
        "\n",
        "# Inject anomalies that reflect real-world issues (~5% anomalies)\n",
        "anomaly_indices = np.random.choice(df.index, size=int(0.05 * N), replace=False)\n",
        "df['Is_Anomaly'] = 0\n",
        "df.loc[anomaly_indices, 'Is_Anomaly'] = 1\n",
        "\n",
        "# Types of anomalies to inject:\n",
        "# 1) Extremely large exposure spike (corporate/bank) for some anomalies\n",
        "spike_idx = anomaly_indices[:int(0.35*len(anomaly_indices))]\n",
        "df.loc[spike_idx, 'Exposure_Amount'] = df.loc[spike_idx, 'Exposure_Amount'] * np.random.randint(10,80, size=len(spike_idx))\n",
        "df.loc[spike_idx, 'Capital_Requirement'] = (df.loc[spike_idx, 'Exposure_Amount'] * df.loc[spike_idx, 'Risk_Weight'] * 0.08).round(0).astype(int)\n",
        "\n",
        "# 2) Invalid currency codes for some anomalies\n",
        "invcur_idx = anomaly_indices[int(0.35*len(anomaly_indices)):int(0.6*len(anomaly_indices))]\n",
        "df.loc[invcur_idx, 'Currency'] = np.random.choice(['XXX','ZZZ','N/A'], size=len(invcur_idx))\n",
        "\n",
        "# 3) Negative exposures (data-entry error)\n",
        "neg_idx = anomaly_indices[int(0.6*len(anomaly_indices)):int(0.8*len(anomaly_indices))]\n",
        "df.loc[neg_idx, 'Exposure_Amount'] = -abs(df.loc[neg_idx, 'Exposure_Amount'])\n",
        "df.loc[neg_idx, 'Capital_Requirement'] = (df.loc[neg_idx, 'Exposure_Amount'] * df.loc[neg_idx, 'Risk_Weight'] * 0.08).round(0).astype(int)\n",
        "\n",
        "# 4) Missing critical fields for some anomalies\n",
        "miss_idx = anomaly_indices[int(0.8*len(anomaly_indices)):]\n",
        "df.loc[miss_idx, np.random.choice(['Currency','Risk_Weight','Country_Code'], size=len(miss_idx))] = None\n",
        "\n",
        "# Duplicate some rows to emulate ingestion duplicates (~1% extra duplicates)\n",
        "dup_idx = np.random.choice(df.index, size=int(0.01 * N), replace=False)\n",
        "df = pd.concat([df, df.loc[dup_idx]], ignore_index=True).reset_index(drop=True)\n",
        "\n",
        "# Recompute flags\n",
        "df['Missing_Flag'] = df.isnull().any(axis=1).astype(int)\n",
        "df['Duplicate_Flag'] = df.duplicated(subset=['Account_ID','Report_Date','Exposure_Amount'], keep=False).astype(int)\n",
        "\n",
        "# Basic profiling summary\n",
        "N_total = len(df)\n",
        "profile = {\n",
        "    'Total_Records': N_total,\n",
        "    'Total_Accounts': df['Account_ID'].nunique(),\n",
        "    'Missing_Percentage': round(df['Missing_Flag'].sum() / N_total * 100, 3),\n",
        "    'Duplicate_Percentage': round(df['Duplicate_Flag'].sum() / N_total * 100, 3),\n",
        "    'Labeled_Anomalies': int(df['Is_Anomaly'].sum()),\n",
        "    'Negative_Exposures': int((df['Exposure_Amount'] <= 0).sum())\n",
        "}\n",
        "\n",
        "# Save CSV\n",
        "os.makedirs('/mnt/data', exist_ok=True)\n",
        "file_path = '/mnt/data/realistic_regulatory_data.csv'\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "# Show profile and sample\n",
        "print(\"Realistic regulatory-style dataset generated with {} records.\".format(N_total))\n",
        "print(\"Saved to:\", file_path)\n",
        "print(\"\\nBasic Data Profile:\")\n",
        "for k,v in profile.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "# Display top rows\n",
        "df.head(10).to_csv('/mnt/data/realistic_sample_top10.csv', index=False)\n",
        "file_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "2If_TTnFq0-F",
        "outputId": "4c7e5951-f519-43fa-e8a4-50ea1b80055e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Realistic regulatory-style dataset generated with 7070 records.\n",
            "Saved to: /mnt/data/realistic_regulatory_data.csv\n",
            "\n",
            "Basic Data Profile:\n",
            "Total_Records: 7070\n",
            "Total_Accounts: 7000\n",
            "Missing_Percentage: 53.239\n",
            "Duplicate_Percentage: 1.98\n",
            "Labeled_Anomalies: 356\n",
            "Negative_Exposures: 73\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/data/realistic_regulatory_data.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regenerating realistic regulatory-style dataset (~7000 rows) and saving as CSV.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random, os\n",
        "\n",
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "\n",
        "N = 7000  # medium size dataset\n",
        "\n",
        "# Account IDs\n",
        "account_ids = [f\"CUST-{500000 + i}\" for i in range(N)]\n",
        "\n",
        "# Report dates spanning 2023-2025 quarterly periods\n",
        "start = datetime(2023,1,1)\n",
        "report_dates = [start + timedelta(days=int(x)) for x in np.random.randint(0, 1000, size=N)]\n",
        "report_dates = pd.to_datetime(report_dates).date\n",
        "\n",
        "# Exposure amounts\n",
        "exposure_amount = np.round(np.where(\n",
        "    np.random.rand(N) < 0.7,\n",
        "    np.random.lognormal(mean=12, sigma=1.0, size=N),\n",
        "    np.random.normal(loc=2000000, scale=800000, size=N)\n",
        ")).astype(int)\n",
        "exposure_amount = np.where(exposure_amount < 100, np.abs(exposure_amount) + 100, exposure_amount)\n",
        "\n",
        "# Counterparty types and derived risk weights\n",
        "counterparty_types = np.random.choice(['Bank','Corporate','Retail','Sovereign','SME'], size=N, p=[0.15,0.35,0.35,0.03,0.12])\n",
        "\n",
        "# Country codes and currency mapping\n",
        "country_codes = np.random.choice(['GB','DE','HK','US','FR','IN','SG','ES','IT','NL'], size=N,\n",
        "                                 p=[0.15,0.12,0.10,0.15,0.10,0.12,0.06,0.05,0.08,0.07])\n",
        "currency_map = {'IN':'INR','GB':'GBP','DE':'EUR','HK':'HKD','US':'USD','FR':'EUR','SG':'SGD','ES':'EUR','IT':'EUR','NL':'EUR'}\n",
        "currency = [currency_map[c] for c in country_codes]\n",
        "\n",
        "# Credit score proxies\n",
        "credit_score = np.where(np.isin(counterparty_types, ['Retail','SME']),\n",
        "                        np.random.normal(loc=650, scale=80, size=N).astype(int), np.nan)\n",
        "credit_score = np.clip(credit_score, 300, 850)\n",
        "credit_score = np.where(np.isnan(credit_score), None, credit_score)\n",
        "\n",
        "# Tenor and transaction count\n",
        "tenor_months = np.random.choice([6,12,24,36,48,60,120], size=N, p=[0.05,0.1,0.2,0.25,0.2,0.15,0.05])\n",
        "transaction_count = np.random.poisson(lam=4, size=N)\n",
        "\n",
        "# Risk weight baseline\n",
        "rw_map = {'Retail': 0.35, 'Corporate': 0.5, 'Bank': 0.2, 'Sovereign': 0.0, 'SME': 0.75}\n",
        "risk_weight = np.array([rw_map[ct] for ct in counterparty_types])\n",
        "\n",
        "# Country-specific upweighting\n",
        "country_up = {'IN': 1.2, 'HK': 1.0, 'US': 1.0, 'GB': 1.0, 'DE': 1.0, 'FR':1.0, 'SG':1.0, 'ES':1.0, 'IT':1.1, 'NL':1.0}\n",
        "risk_weight = risk_weight * np.array([country_up[c] for c in country_codes])\n",
        "risk_weight = np.clip(risk_weight, 0.0, 1.0).round(2)\n",
        "\n",
        "# Capital requirement\n",
        "capital_requirement = (exposure_amount * risk_weight * 0.08).round(0).astype(int)\n",
        "\n",
        "# Source system\n",
        "source_system = np.random.choice(['CoreBank','LoanServicing','Treasury','TradeRepo','ExternalFeed'], size=N, p=[0.4,0.25,0.15,0.1,0.1])\n",
        "\n",
        "# DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Account_ID': account_ids,\n",
        "    'Report_Date': report_dates,\n",
        "    'Exposure_Amount': exposure_amount,\n",
        "    'Risk_Weight': risk_weight,\n",
        "    'Capital_Requirement': capital_requirement,\n",
        "    'Country_Code': country_codes,\n",
        "    'Currency': currency,\n",
        "    'Counterparty_Type': counterparty_types,\n",
        "    'Tenor_Months': tenor_months,\n",
        "    'Transaction_Count': transaction_count,\n",
        "    'Source_System': source_system,\n",
        "    'Credit_Score': credit_score\n",
        "})\n",
        "\n",
        "df['Product_Type'] = np.random.choice(['TermLoan','Overdraft','CashCredit','TradeFinance','Repo'], size=N, p=[0.3,0.25,0.2,0.15,0.1])\n",
        "df['Facility_ID'] = ['FAC-' + str(700000 + i) for i in range(len(df))]\n",
        "\n",
        "# Inject anomalies (~5%)\n",
        "anomaly_indices = np.random.choice(df.index, size=int(0.05 * N), replace=False)\n",
        "df['Is_Anomaly'] = 0\n",
        "df.loc[anomaly_indices, 'Is_Anomaly'] = 1\n",
        "\n",
        "# 1) Exposure spikes\n",
        "spike_idx = anomaly_indices[:int(0.35*len(anomaly_indices))]\n",
        "df.loc[spike_idx, 'Exposure_Amount'] = df.loc[spike_idx, 'Exposure_Amount'] * np.random.randint(10,80, size=len(spike_idx))\n",
        "df.loc[spike_idx, 'Capital_Requirement'] = (df.loc[spike_idx, 'Exposure_Amount'] * df.loc[spike_idx, 'Risk_Weight'] * 0.08).round(0).astype(int)\n",
        "\n",
        "# 2) Invalid currency codes\n",
        "invcur_idx = anomaly_indices[int(0.35*len(anomaly_indices)):int(0.6*len(anomaly_indices))]\n",
        "df.loc[invcur_idx, 'Currency'] = np.random.choice(['XXX','ZZZ','N/A'], size=len(invcur_idx))\n",
        "\n",
        "# 3) Negative exposures\n",
        "neg_idx = anomaly_indices[int(0.6*len(anomaly_indices)):int(0.8*len(anomaly_indices))]\n",
        "df.loc[neg_idx, 'Exposure_Amount'] = -abs(df.loc[neg_idx, 'Exposure_Amount'])\n",
        "df.loc[neg_idx, 'Capital_Requirement'] = (df.loc[neg_idx, 'Exposure_Amount'] * df.loc[neg_idx, 'Risk_Weight'] * 0.08).round(0).astype(int)\n",
        "\n",
        "# 4) Missing critical fields\n",
        "miss_idx = anomaly_indices[int(0.8*len(anomaly_indices)):]\n",
        "cols_for_missing = ['Currency','Risk_Weight','Country_Code']\n",
        "for i, idx in enumerate(miss_idx):\n",
        "    col = cols_for_missing[i % len(cols_for_missing)]\n",
        "    df.at[idx, col] = None\n",
        "\n",
        "# Duplicates (~1%)\n",
        "dup_idx = np.random.choice(df.index, size=int(0.01 * N), replace=False)\n",
        "df = pd.concat([df, df.loc[dup_idx]], ignore_index=True).reset_index(drop=True)\n",
        "\n",
        "# Flags\n",
        "df['Missing_Flag'] = df.isnull().any(axis=1).astype(int)\n",
        "df['Duplicate_Flag'] = df.duplicated(subset=['Account_ID','Report_Date','Exposure_Amount'], keep=False).astype(int)\n",
        "\n",
        "# Save CSV\n",
        "os.makedirs('/mnt/data', exist_ok=True)\n",
        "file_path = '/mnt/data/realistic_regulatory_data.csv'\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "# Summary\n",
        "N_total = len(df)\n",
        "profile = {\n",
        "    'Total_Records': N_total,\n",
        "    'Total_Accounts': df['Account_ID'].nunique(),\n",
        "    'Missing_Percentage': round(df['Missing_Flag'].sum() / N_total * 100, 3),\n",
        "    'Duplicate_Percentage': round(df['Duplicate_Flag'].sum() / N_total * 100, 3),\n",
        "    'Labeled_Anomalies': int(df['Is_Anomaly'].sum()),\n",
        "    'Negative_Exposures': int((df['Exposure_Amount'] <= 0).sum())\n",
        "}\n",
        "\n",
        "print(\"Realistic regulatory-style dataset generated with {} records.\".format(N_total))\n",
        "print(\"Saved to:\", file_path)\n",
        "print(\"\\nBasic Data Profile:\")\n",
        "for k,v in profile.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "# Save sample top 10\n",
        "df.head(10).to_csv('/mnt/data/realistic_sample_top10.csv', index=False)\n",
        "\n",
        "file_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "kjQpWvpY5lvv",
        "outputId": "8eabbbad-e2d6-4fd4-84e2-e7a42147f34a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Realistic regulatory-style dataset generated with 7070 records.\n",
            "Saved to: /mnt/data/realistic_regulatory_data.csv\n",
            "\n",
            "Basic Data Profile:\n",
            "Total_Records: 7070\n",
            "Total_Accounts: 7000\n",
            "Missing_Percentage: 53.267\n",
            "Duplicate_Percentage: 1.98\n",
            "Labeled_Anomalies: 351\n",
            "Negative_Exposures: 70\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/data/realistic_regulatory_data.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-process dataset to recompute Missing_Flag based on critical fields only\n",
        "import pandas as pd\n",
        "file_path = '/mnt/data/realistic_regulatory_data.csv'\n",
        "df = pd.read_csv(file_path, parse_dates=['Report_Date'])\n",
        "\n",
        "# Define critical fields for DQ: Exposure_Amount, Risk_Weight, Currency, Country_Code\n",
        "critical_fields = ['Exposure_Amount','Risk_Weight','Currency','Country_Code']\n",
        "\n",
        "df['Missing_Flag'] = df[critical_fields].isnull().any(axis=1).astype(int)\n",
        "df['Duplicate_Flag'] = df.duplicated(subset=['Account_ID','Report_Date','Exposure_Amount'], keep=False).astype(int)\n",
        "\n",
        "# Recompute basic profile\n",
        "N_total = len(df)\n",
        "profile = {\n",
        "    'Total_Records': N_total,\n",
        "    'Total_Accounts': df['Account_ID'].nunique(),\n",
        "    'Missing_Percentage': round(df['Missing_Flag'].sum() / N_total * 100, 3),\n",
        "    'Duplicate_Percentage': round(df['Duplicate_Flag'].sum() / N_total * 100, 3),\n",
        "    'Labeled_Anomalies': int(df['Is_Anomaly'].sum()),\n",
        "    'Negative_Exposures': int((df['Exposure_Amount'] <= 0).sum())\n",
        "}\n",
        "\n",
        "# Save updated CSV and sample\n",
        "df.to_csv(file_path, index=False)\n",
        "df.head(10).to_csv('/mnt/data/realistic_sample_top10.csv', index=False)\n",
        "\n",
        "print(\"Recomputed Missing_Flag on critical fields and updated file:\", file_path)\n",
        "print(\"\\nUpdated Basic Data Profile:\")\n",
        "for k,v in profile.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "file_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "DAjAH1TF5sLH",
        "outputId": "22484bd5-dfc6-489b-d68e-76cef16b33b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recomputed Missing_Flag on critical fields and updated file: /mnt/data/realistic_regulatory_data.csv\n",
            "\n",
            "Updated Basic Data Profile:\n",
            "Total_Records: 7070\n",
            "Total_Accounts: 7000\n",
            "Missing_Percentage: 1.429\n",
            "Duplicate_Percentage: 1.98\n",
            "Labeled_Anomalies: 351\n",
            "Negative_Exposures: 70\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/data/realistic_regulatory_data.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGyz8XIC90rz",
        "outputId": "32bd385f-6224-4d63-ec1c-f5e32480e86d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset: /mnt/data/realistic_regulatory_data.csv shape: (7070, 17)\n",
            "Train/test shapes: (4949, 10) (2121, 10) Label counts in test: [2016  105]\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "=== MODELING SUMMARY ===\n",
            "Train shape: (4949, 40) Test shape: (2121, 40)\n",
            "Label count (test): 105\n",
            "ROC AUC results (if labels present):\n",
            "  IsolationForest_ROC_AUC: 0.6491118669690097\n",
            "  LOF_ROC_AUC: 0.8341883975812547\n",
            "  OCSVM_ROC_AUC: 0.6223922902494331\n",
            "  Autoencoder_ROC_AUC: 0.7357615268329554\n",
            "Precision@5% for models:\n",
            "  IF_Score_prec_at_5pct: 0.179\n",
            "  LOF_Score_prec_at_5pct: 0.462\n",
            "  OCSVM_Score_prec_at_5pct: 0.189\n",
            "  AE_ReconScore_prec_at_5pct: 0.274\n",
            "\n",
            "Artifacts saved to /mnt/data/:\n",
            " - model_scores_comparison_realistic.csv\n",
            " - autoencoder_training_loss_realistic.png\n",
            " - shap_values_if_realistic.npy\n",
            " - autoencoder_perm_importance_realistic.csv\n",
            " - models/ (preprocessor + trained models)\n",
            " - modeling_summary_realistic.pkl\n"
          ]
        }
      ],
      "source": [
        "# Running model-comparison on the final realistic_regulatory_data.csv\n",
        "# This cell trains IsolationForest, Autoencoder, LOF, One-Class SVM on the dataset,\n",
        "# computes scores, evaluates (ROC AUC), computes SHAP for IsolationForest (sample),\n",
        "# computes permutation importance for Autoencoder (approx), and saves artifacts.\n",
        "# Outputs summary metrics below and saves CSVs/plots to /mnt/data.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import shap\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load dataset\n",
        "csv_path = \"/mnt/data/realistic_regulatory_data.csv\"\n",
        "df = pd.read_csv(csv_path, parse_dates=['Report_Date'])\n",
        "print(\"Loaded dataset:\", csv_path, \"shape:\", df.shape)\n",
        "\n",
        "# Select features: numeric + categorical for modelling\n",
        "numeric_features = ['Exposure_Amount', 'Risk_Weight', 'Capital_Requirement', 'Tenor_Months', 'Transaction_Count']\n",
        "categorical_features = ['Country_Code', 'Currency', 'Counterparty_Type', 'Source_System', 'Product_Type']\n",
        "\n",
        "# Prepare labels\n",
        "if 'Is_Anomaly' in df.columns:\n",
        "    y = df['Is_Anomaly'].astype(int).values\n",
        "else:\n",
        "    y = np.zeros(len(df))\n",
        "\n",
        "# Fill NA\n",
        "df[numeric_features] = df[numeric_features].fillna(0)\n",
        "df[categorical_features] = df[categorical_features].fillna('MISSING')\n",
        "\n",
        "X = df[numeric_features + categorical_features].copy()\n",
        "\n",
        "# Train-test split (stratify if labels present)\n",
        "if y.sum() > 0:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
        "else:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, test_size=0.30, random_state=42)\n",
        "\n",
        "print(\"Train/test shapes:\", X_train.shape, X_test.shape, \"Label counts in test:\", np.bincount(y_test))\n",
        "\n",
        "# Preprocessing\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numeric_features),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
        "])\n",
        "\n",
        "preprocessor.fit(X_train)\n",
        "X_train_proc = preprocessor.transform(X_train)\n",
        "X_test_proc = preprocessor.transform(X_test)\n",
        "\n",
        "# Save preprocessor\n",
        "os.makedirs('/mnt/data/models', exist_ok=True)\n",
        "joblib.dump(preprocessor, '/mnt/data/models/preprocessor_realistic.joblib')\n",
        "\n",
        "# Isolation Forest\n",
        "if_model = IsolationForest(n_estimators=200, contamination=0.05, random_state=42)\n",
        "if_model.fit(X_train_proc)\n",
        "if_scores_test = -if_model.decision_function(X_test_proc)\n",
        "if_scores_test = (if_scores_test - if_scores_test.min()) / (if_scores_test.max() - if_scores_test.min())\n",
        "joblib.dump(if_model, '/mnt/data/models/isolation_forest_realistic.joblib')\n",
        "\n",
        "# LOF novelty True\n",
        "lof = LocalOutlierFactor(n_neighbors=35, contamination=0.05, novelty=True)\n",
        "lof.fit(X_train_proc)\n",
        "lof_scores_test = -lof.decision_function(X_test_proc)\n",
        "lof_scores_test = (lof_scores_test - lof_scores_test.min()) / (lof_scores_test.max() - lof_scores_test.min())\n",
        "joblib.dump(lof, '/mnt/data/models/lof_realistic.joblib')\n",
        "\n",
        "# One-Class SVM\n",
        "ocsvm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.05)\n",
        "ocsvm.fit(X_train_proc)\n",
        "ocsvm_scores_test = -ocsvm.decision_function(X_test_proc)\n",
        "ocsvm_scores_test = (ocsvm_scores_test - ocsvm_scores_test.min()) / (ocsvm_scores_test.max() - ocsvm_scores_test.min())\n",
        "joblib.dump(ocsvm, '/mnt/data/models/ocsvm_realistic.joblib')\n",
        "\n",
        "# Autoencoder\n",
        "input_dim = X_train_proc.shape[1]\n",
        "encoding_dim = max(8, int(input_dim / 6))\n",
        "\n",
        "autoencoder = keras.Sequential([\n",
        "    layers.Input(shape=(input_dim,)),\n",
        "    layers.Dense(encoding_dim*2, activation='relu'),\n",
        "    layers.Dense(encoding_dim, activation='relu'),\n",
        "    layers.Dense(encoding_dim*2, activation='relu'),\n",
        "    layers.Dense(input_dim, activation='linear')\n",
        "])\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "history = autoencoder.fit(X_train_proc, X_train_proc, epochs=20, batch_size=256, validation_split=0.1, verbose=0)\n",
        "X_test_pred = autoencoder.predict(X_test_proc)\n",
        "recon_error = np.mean(np.square(X_test_proc - X_test_pred), axis=1)\n",
        "recon_scores_test = (recon_error - recon_error.min()) / (recon_error.max() - recon_error.min())\n",
        "autoencoder.save('/mnt/data/models/autoencoder_realistic.h5')\n",
        "\n",
        "# Save training loss plot\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.title('Autoencoder Training Loss')\n",
        "plt.savefig('/mnt/data/autoencoder_training_loss_realistic.png', bbox_inches='tight', dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# Compile scores DataFrame\n",
        "scores_df = pd.DataFrame({\n",
        "    'Is_Anomaly': y_test,\n",
        "    'IF_Score': if_scores_test,\n",
        "    'LOF_Score': lof_scores_test,\n",
        "    'OCSVM_Score': ocsvm_scores_test,\n",
        "    'AE_ReconScore': recon_scores_test\n",
        "})\n",
        "\n",
        "scores_df.to_csv('/mnt/data/model_scores_comparison_realistic.csv', index=False)\n",
        "\n",
        "# Evaluation - ROC AUC\n",
        "results = {}\n",
        "if y_test.sum() > 0:\n",
        "    results['IsolationForest_ROC_AUC'] = roc_auc_score(y_test, if_scores_test)\n",
        "    results['LOF_ROC_AUC'] = roc_auc_score(y_test, lof_scores_test)\n",
        "    results['OCSVM_ROC_AUC'] = roc_auc_score(y_test, ocsvm_scores_test)\n",
        "    results['Autoencoder_ROC_AUC'] = roc_auc_score(y_test, recon_scores_test)\n",
        "else:\n",
        "    results['Warning'] = 'No positive labels in test set.'\n",
        "\n",
        "# Precision@k (top 5%)\n",
        "k = int(0.05 * len(scores_df))\n",
        "precisions = {}\n",
        "for col in ['IF_Score','LOF_Score','OCSVM_Score','AE_ReconScore']:\n",
        "    topk = scores_df.nlargest(k, col)\n",
        "    if topk['Is_Anomaly'].sum() == 0:\n",
        "        precisions[col+'_prec_at_5pct'] = 0.0\n",
        "    else:\n",
        "        precisions[col+'_prec_at_5pct'] = topk['Is_Anomaly'].sum() / k\n",
        "\n",
        "# SHAP for Isolation Forest (use small background sample)\n",
        "explainer = shap.TreeExplainer(if_model)\n",
        "n_shap = min(200, X_test_proc.shape[0])\n",
        "shap_vals = explainer.shap_values(X_test_proc[:n_shap])\n",
        "np.save('/mnt/data/shap_values_if_realistic.npy', shap_vals)\n",
        "\n",
        "# Permutation importance for Autoencoder (approx)\n",
        "base_mse = np.mean(np.square(X_test_proc - X_test_pred), axis=1).mean()\n",
        "perm_imp = []\n",
        "for col in range(X_test_proc.shape[1]):\n",
        "    X_perm = X_test_proc.copy()\n",
        "    np.random.shuffle(X_perm[:, col])\n",
        "    pred = autoencoder.predict(X_perm)\n",
        "    mse_perm = np.mean(np.square(X_perm - pred), axis=1).mean()\n",
        "    perm_imp.append(mse_perm - base_mse)\n",
        "\n",
        "# Map feature names\n",
        "ohe = preprocessor.named_transformers_['cat']\n",
        "cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
        "feature_names = numeric_features + cat_names\n",
        "feature_names = feature_names[:len(perm_imp)]\n",
        "perm_df = pd.DataFrame({'feature': feature_names, 'perm_importance': perm_imp})\n",
        "perm_df.sort_values('perm_importance', ascending=False, inplace=True)\n",
        "perm_df.to_csv('/mnt/data/autoencoder_perm_importance_realistic.csv', index=False)\n",
        "\n",
        "# Save summary and results\n",
        "summary = {\n",
        "    'train_shape': X_train_proc.shape,\n",
        "    'test_shape': X_test_proc.shape,\n",
        "    'label_counts_test': int(y_test.sum()),\n",
        "    'results': results,\n",
        "    'precisions_top5pct': precisions\n",
        "}\n",
        "pd.to_pickle(summary, '/mnt/data/modeling_summary_realistic.pkl')\n",
        "\n",
        "# Print concise summary\n",
        "print(\"=== MODELING SUMMARY ===\")\n",
        "print(\"Train shape:\", X_train_proc.shape, \"Test shape:\", X_test_proc.shape)\n",
        "print(\"Label count (test):\", int(y_test.sum()))\n",
        "print(\"ROC AUC results (if labels present):\")\n",
        "for k,v in results.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(\"Precision@5% for models:\")\n",
        "for k,v in precisions.items():\n",
        "    print(f\"  {k}: {v:.3f}\")\n",
        "\n",
        "print(\"\\nArtifacts saved to /mnt/data/:\")\n",
        "print(\" - model_scores_comparison_realistic.csv\")\n",
        "print(\" - autoencoder_training_loss_realistic.png\")\n",
        "print(\" - shap_values_if_realistic.npy\")\n",
        "print(\" - autoencoder_perm_importance_realistic.csv\")\n",
        "print(\" - models/ (preprocessor + trained models)\")\n",
        "print(\" - modeling_summary_realistic.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Quality Rule Engine module\n",
        "# This script reads a regulatory-style CSV, applies deterministic data-quality rules aligned to PRA/EBA/ECB,\n",
        "# and outputs a flagged dataset and summary KPIs.\n",
        "# Save this cell as dq_rule_engine.py or run directly in Colab/Jupyter.\n",
        "#\n",
        "# Usage example (in Colab):\n",
        "# 1. Upload / copy your CSV to /content/ or /mnt/data/\n",
        "# 2. Adjust CSV_PATH below or pass path as argument\n",
        "# 3. Run the script. It will save dq_report.csv and dq_summary.json in the same directory.\n",
        "#\n",
        "# The code is intentionally self-contained and does not require special packages beyond pandas & numpy.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict\n",
        "\n",
        "# ------------------------------ Configuration ------------------------------\n",
        "# Allowed code-lists and simple business rules. Edit as needed for your environment.\n",
        "ALLOWED_CURRENCIES = {'INR','USD','GBP','EUR','HKD','SGD'}\n",
        "ISO_COUNTRIES = {'GB','DE','HK','US','FR','IN','SG','ES','IT','NL'}  # subset used in dataset\n",
        "CURRENCY_BY_COUNTRY = {\n",
        "    'IN':'INR','GB':'GBP','DE':'EUR','HK':'HKD','US':'USD','FR':'EUR','SG':'SGD','ES':'EUR','IT':'EUR','NL':'EUR'\n",
        "}\n",
        "CRITICAL_FIELDS = ['Exposure_Amount','Risk_Weight','Currency','Country_Code','Counterparty_Type','Report_Date']\n",
        "\n",
        "# Thresholds / bounds\n",
        "MIN_EXPOSURE = 0  # exposures must be >= 0\n",
        "RISK_WEIGHT_MIN, RISK_WEIGHT_MAX = 0.0, 1.0\n",
        "TENOR_MIN, TENOR_MAX = 1, 360\n",
        "CAPITAL_TOLERANCE_PCT = 0.05  # 5% tolerance for capital recomputation check\n",
        "\n",
        "# Duplicate key definition\n",
        "DUP_KEY = ['Account_ID','Report_Date','Exposure_Amount']\n",
        "\n",
        "# ------------------------------ Utility functions ------------------------------\n",
        "def load_data(csv_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load CSV into dataframe and ensure report date is parsed.\"\"\"\n",
        "    df = pd.read_csv(csv_path, parse_dates=['Report_Date'], dayfirst=True, low_memory=False)\n",
        "    return df\n",
        "\n",
        "def safe_div(a, b):\n",
        "    try:\n",
        "        return a / b\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "# ------------------------------ Rule implementations ------------------------------\n",
        "def check_completeness(df: pd.DataFrame, fields: List[str]=CRITICAL_FIELDS) -> pd.Series:\n",
        "    \"\"\"Return binary flag: 1 if any critical field is missing, else 0.\"\"\"\n",
        "    return df[fields].isnull().any(axis=1).astype(int)\n",
        "\n",
        "def check_currency_validity(df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"Flag records where currency is missing or not in allowed list.\"\"\"\n",
        "    return (~df['Currency'].isin(ALLOWED_CURRENCIES)).astype(int)\n",
        "\n",
        "def check_country_validity(df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"Flag records with country codes outside ISO_COUNTRIES subset.\"\"\"\n",
        "    return (~df['Country_Code'].isin(ISO_COUNTRIES)).astype(int)\n",
        "\n",
        "def check_currency_country_mismatch(df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"Flag where country->currency mapping does not match expected mapping (if both present).\"\"\"\n",
        "    def mismatch(row):\n",
        "        c = row.get('Country_Code')\n",
        "        cur = row.get('Currency')\n",
        "        if pd.isna(c) or pd.isna(cur):\n",
        "            return 0\n",
        "        expected = CURRENCY_BY_COUNTRY.get(c)\n",
        "        if expected is None:\n",
        "            return 0\n",
        "        return int(cur != expected)\n",
        "    return df.apply(mismatch, axis=1)\n",
        "\n",
        "def check_negative_exposure(df: pd.DataFrame) -> pd.Series:\n",
        "    return (df['Exposure_Amount'] < MIN_EXPOSURE).astype(int)\n",
        "\n",
        "def check_risk_weight_bounds(df: pd.DataFrame) -> pd.Series:\n",
        "    return (~df['Risk_Weight'].between(RISK_WEIGHT_MIN, RISK_WEIGHT_MAX)).astype(int)\n",
        "\n",
        "def check_tenor_bounds(df: pd.DataFrame) -> pd.Series:\n",
        "    if 'Tenor_Months' in df.columns:\n",
        "        return (~df['Tenor_Months'].between(TENOR_MIN, TENOR_MAX)).astype(int)\n",
        "    else:\n",
        "        return pd.Series(0, index=df.index)\n",
        "\n",
        "def check_capital_consistency(df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"Check if Capital_Requirement roughly equals Exposure * RW * 8% (within tolerance).\"\"\"\n",
        "    # Compute expected capital (float) and compare with reported if present\n",
        "    exp_cap = df['Exposure_Amount'] * df['Risk_Weight'] * 0.08\n",
        "    reported = df['Capital_Requirement'].fillna(-1)\n",
        "    # Where reported <=0 treat as inconsistent unless both zero\n",
        "    # Compute relative diff safely\n",
        "    rel_diff = (reported - exp_cap).abs() / (exp_cap.replace(0, np.nan).abs())\n",
        "    # Flag if relative diff > tolerance or reported negative while expected positive\n",
        "    flag = ((rel_diff > CAPITAL_TOLERANCE_PCT) & (~exp_cap.isnull())).fillna(False)\n",
        "    # also flag when expected>0 and reported<=0\n",
        "    flag2 = ((exp_cap > 0) & (reported <= 0))\n",
        "    return (flag | flag2).astype(int)\n",
        "\n",
        "def check_duplicates(df: pd.DataFrame, key: List[str]=DUP_KEY) -> pd.Series:\n",
        "    return df.duplicated(subset=key, keep=False).astype(int)\n",
        "\n",
        "def check_outlier_simple(df: pd.DataFrame, multiple: float=10.0) -> pd.Series:\n",
        "    \"\"\"Simple outlier heuristic: exposure > multiple * median(exposure by country)\"\"\"\n",
        "    med = df.groupby('Country_Code')['Exposure_Amount'].transform('median').replace(0, np.nan)\n",
        "    return (df['Exposure_Amount'].abs() > multiple * med).fillna(0).astype(int)\n",
        "\n",
        "# ------------------------------ Main engine ------------------------------\n",
        "def apply_dq_rules(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Apply all DQ rules and return dataframe with new flag columns and a DQ_Score.\"\"\"\n",
        "    df = df.copy()\n",
        "    # Ensure critical numeric columns exist\n",
        "    for col in ['Exposure_Amount','Risk_Weight','Capital_Requirement']:\n",
        "        if col not in df.columns:\n",
        "            df[col] = np.nan\n",
        "    # Completeness\n",
        "    df['DQ_Missing_Flag'] = check_completeness(df)\n",
        "    # Validity checks\n",
        "    df['DQ_Invalid_Currency_Flag'] = check_currency_validity(df)\n",
        "    df['DQ_Invalid_Country_Flag'] = check_country_validity(df)\n",
        "    df['DQ_Ccy_Ctry_Mismatch_Flag'] = check_currency_country_mismatch(df)\n",
        "    # Numeric & threshold checks\n",
        "    df['DQ_Negative_Exposure_Flag'] = check_negative_exposure(df)\n",
        "    df['DQ_RiskWeight_Flag'] = check_risk_weight_bounds(df)\n",
        "    df['DQ_Tenor_Flag'] = check_tenor_bounds(df)\n",
        "    df['DQ_Capital_Inconsistency_Flag'] = check_capital_consistency(df)\n",
        "    # Duplicates & outliers\n",
        "    df['DQ_Duplicate_Flag'] = check_duplicates(df)\n",
        "    df['DQ_Exposure_Outlier_Flag'] = check_outlier_simple(df)\n",
        "    # Combine into an overall DQ Rule-based score (weighted sum)\n",
        "    # Weights are configurable — here we use equal weight for simplicity\n",
        "    flag_cols = [c for c in df.columns if c.startswith('DQ_') and c.endswith('_Flag')]\n",
        "    df['DQ_Rule_Score'] = df[flag_cols].sum(axis=1).astype(int)  # simple count of flags\n",
        "    # Binary rule-based anomaly: if any rule flag or rule score > 0\n",
        "    df['DQ_Rule_Anomaly'] = (df['DQ_Rule_Score'] > 0).astype(int)\n",
        "    return df\n",
        "\n",
        "def compute_dq_kpis(df: pd.DataFrame) -> Dict[str, float]:\n",
        "    \"\"\"Compute summary KPIs for the dataset based on applied DQ flags.\"\"\"\n",
        "    total = len(df)\n",
        "    kpi = {\n",
        "        'Total_Records': int(total),\n",
        "        'Missing_Percentage': float(df['DQ_Missing_Flag'].sum() / total * 100),\n",
        "        'Duplicate_Percentage': float(df['DQ_Duplicate_Flag'].sum() / total * 100),\n",
        "        'Invalid_Currency_Percentage': float(df['DQ_Invalid_Currency_Flag'].sum() / total * 100),\n",
        "        'Invalid_Country_Percentage': float(df['DQ_Invalid_Country_Flag'].sum() / total * 100),\n",
        "        'Negative_Exposure_Count': int(df['DQ_Negative_Exposure_Flag'].sum()),\n",
        "        'Capital_Inconsistency_Count': int(df['DQ_Capital_Inconsistency_Flag'].sum()),\n",
        "        'Exposure_Outlier_Percentage': float(df['DQ_Exposure_Outlier_Flag'].sum() / total * 100),\n",
        "        'Rule_Anomaly_Percentage': float(df['DQ_Rule_Anomaly'].sum() / total * 100),\n",
        "    }\n",
        "    return kpi\n",
        "\n",
        "def save_outputs(df: pd.DataFrame, out_csv: str, summary_json: str=None):\n",
        "    \"\"\"Save flagged dataframe and optional summary JSON to disk.\"\"\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"Saved flagged dataset to: {out_csv}\")\n",
        "    if summary_json:\n",
        "        kpi = compute_dq_kpis(df)\n",
        "        with open(summary_json, 'w') as f:\n",
        "            json.dump(kpi, f, indent=2)\n",
        "        print(f\"Saved DQ summary to: {summary_json}\")\n",
        "        return kpi\n",
        "    return None\n",
        "\n",
        "# ------------------------------ Example script entrypoint ------------------------------\n",
        "if __name__ == '__main__':\n",
        "    # Default paths (edit if running locally)\n",
        "    INPUT_CSV = '/mnt/data/realistic_regulatory_data.csv'\n",
        "    OUTPUT_CSV = '/mnt/data/dq_report_realistic.csv'\n",
        "    SUMMARY_JSON = '/mnt/data/dq_summary_realistic.json'\n",
        "\n",
        "    if not os.path.exists(INPUT_CSV):\n",
        "        print(\"Input CSV not found at\", INPUT_CSV)\n",
        "    else:\n",
        "        df_in = load_data(INPUT_CSV)\n",
        "        df_flagged = apply_dq_rules(df_in)\n",
        "        kpi = save_outputs(df_flagged, OUTPUT_CSV, SUMMARY_JSON)\n",
        "        print(\"\\nDQ KPIs:\")\n",
        "        print(json.dumps(kpi, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMlnJtjtPOua",
        "outputId": "ac34ca7b-d0f9-499a-f410-28958ee52c8e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved flagged dataset to: /mnt/data/dq_report_realistic.csv\n",
            "Saved DQ summary to: /mnt/data/dq_summary_realistic.json\n",
            "\n",
            "DQ KPIs:\n",
            "{\n",
            "  \"Total_Records\": 7070,\n",
            "  \"Missing_Percentage\": 1.4285714285714286,\n",
            "  \"Duplicate_Percentage\": 1.9801980198019802,\n",
            "  \"Invalid_Currency_Percentage\": 1.5841584158415842,\n",
            "  \"Invalid_Country_Percentage\": 0.32531824611032534,\n",
            "  \"Negative_Exposure_Count\": 70,\n",
            "  \"Capital_Inconsistency_Count\": 0,\n",
            "  \"Exposure_Outlier_Percentage\": 4.837340876944837,\n",
            "  \"Rule_Anomaly_Percentage\": 9.886845827439886\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}